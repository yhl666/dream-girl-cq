01   import tensorflow as tf
02   from tensorflow.examples.tutorials.mnist import input_data
03   # number 1 to 10 data
04   mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
05   
06   def compute_accuracy(v_xs, v_ys):
07     global prediction
08     y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
09     correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
10     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
11     result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
12     return result
13   
14   def weight_variable(shape):
15     initial = tf.truncated_normal(shape, stddev=0.1)
16     return tf.Variable(initial)
17   
18   def bias_variable(shape):
19     initial = tf.constant(0.1, shape=shape)
20     return tf.Variable(initial)
21   
22   def conv2d(x, W):
23     # stride [1, x_movement, y_movement, 1]
24     # Must have strides[0] = strides[3] = 1
25     return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
26   
27   def max_pool_2x2(x):
28     # stride [1, x_movement, y_movement, 1]
29     return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], \
30                         padding='SAME')
31   # define placeholder for inputs to network
32   xs = tf.placeholder(tf.float32, [None, 784]) # 28x28
33   ys = tf.placeholder(tf.float32, [None, 10])
34   keep_prob = tf.placeholder(tf.float32)
35   x_image = tf.reshape(xs, [-1,28,28,1])
36   #print(x_image.shape) #[n_samples,28,28,1]
37   
38   ## conv1 layer ##
39   W_conv1 = weight_variable([5,5,1,32])
40   b_conv1  bias_variable([32])
41   h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)
42   h_pool1 = max_pool_2x2(h_conv1)             #output size 14x14x32
43   
44   ## conv2 layer ##
45   W_conv2 = weight_variable([5,5,32,64])#patch 5x5,in size 32,out size 64
46   b_conv2 = bias_variable([64])
47   h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)
48   h_pool2 = max_pool_2x2(h_conv2)                #output size 7x7x64
49   
50   ## func1 layer ##
51   W_fc1 = weight_variable([7*7*64,1024])
52   b_fc1 = bias_variable([1024])
53   #[n_samples,7,7,64]->>[n_samples,7*7*64]
54   h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])
55   h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)
56   h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)
57   
58   ## func2 layer ##
59   W_fc2 = weight_variable([1024,10])
60   b_fc2 = bias_variable([10])
61   prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)
62   
63   
64   # the error between prediction and real data
65   cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\
66               reduction_indices=[1]))       # loss
67   train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
68   
69   sess = tf.Session()
70   # important step
71   sess.run(tf.initialize_all_variables())
72   
73   for i in range(1000):
74       batch_xs, batch_ys = mnist.train.next_batch(100)
75       sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys,  \
76                                     keep_prob: 0.5})
77       if i % 200 == 199:
78           print(compute_accuracy(\
79               mnist.test.images, mnist.test.labels))
